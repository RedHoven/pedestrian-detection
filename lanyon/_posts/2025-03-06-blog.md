---
layout: post
title: Detecting Pedestrians on Streets of European Cities
tagline: Object Detection, EuroCityPersons, Safety
---

<!-- why interesting or why people should care. -->

# Introduction

Pedestrian detection is a vital task in computer vision with direct impact on road safety. In busy European cities, where traffic is dense and complex, pedestrians are among the most vulnerable road users. Unlike cyclists or motorcyclists, pedestrians move unpredictably and often appear in unexpected places. This makes their detection a unique and critical challenge, distinct from simply identifying “people” in images.

While many modern vehicles use advanced sensors like LiDAR and radar, these are expensive and hardware-specific. In contrast, camera-based detection offers a low-cost, scalable solution. With only visual input, computer vision models can help identify pedestrians and alert drivers when unexpected behavior occurs — such as sudden crossings.

Given the complex task of pedestrian detection in European cities, we decide to explore three key aspects: robustness, transfer learning, and explainability. For that, we choose to experiment with two state-of-the-art models, from ultralytics library: YOLOv8 ([Jocher et al., 2023](#ref-jocher)) and RT-DETR ([Zhang et al., 2023](#ref-zhang)). In this way we build upon current typical approaches, like bounding box detection algorithms, which use CNN and attention-based models.

## 1. Robustness
We evaluate how pedestrian detection models perform under varying lighting and weather conditions, and suggest ways to improve their reliability.

## 2. Transfer Learning

We evaluate whether knowledge from models trained for pedestrian detection can be effectively transferred to a related task in the traffic domain: detecting traffic signs.

## 3. Explainability
To reduce the “black-box” nature of these models, we visualize their internal decision-making processes and examine how they respond to crowding and occlusion.

## Research Questions

To create a better view of the problems that we want to solve, we propose the following research questions:

1. How do weather and lighting affect model robustness?
2. Can knowledge from pedestrian detection transfer to detecting road signs?
3. What image regions do the selected models rely on across different crowding and occlusion levels?

This blog starts with related work and a description of our dataset and models. We then outline the training setup, present our experiments and results, discuss the results and conclude the research.

[//]: # (462 words)

# Literature Review

Over the past years, different deep learning-based object detection architectures have emerged, among which the You Only Look Once (YOLO) family and the Detection Transformer (DETR) ([Carion et al. (2020)](#ref-carion)) models stand out for their real-time capabilities and performance.

## Evolution of YOLO in Pedestrian Detection

The YOLO series has seen widespread adoption in object detection due to its high speed and accuracy. Introduced by [Redmon et al. (2016)](#ref-redmon), YOLO transformed object detection by treating it as a single regression problem. YOLOv5, for example, offered a lightweight structure suitable for real-time applications.

A more recent model variation, **YOLOv8** has demonstrated enhanced performance through architectural improvements and refined training techniques. It has been applied in real-time scenarios, such as vehicle and pedestrian analysis under dynamic lighting and environmental conditions ([Sadik et al., 2024](#ref-sadik)). YOLOv8 maintains its suitability for urban contexts where detection speed and precision are critical.

## Evolution of DETR in Pedestrian Detection

The **DETR** model, introduced by [Carion et al. (2020)](#ref-carion), brought transformer-based architectures to object detection, enabling end-to-end detection without the need for hand-designed components like anchor boxes or NMS (non-max suppression). Building on this foundation, **RT-DETR** (Real-Time DETR) ([Zhang et al., 2023](#ref-zhang)) aims to retain DETR's benefits while reducing latency for real-time use.

A prominent RT-DETR-based model, **FedsNet**, was developed specifically for pedestrian detection. It achieved improved detection accuracy and reduced computational complexity compared to earlier transformer-based approaches ([Peng & Chen, 2024](#ref-peng)). These characteristics make RT-DETR a compelling choice for fast and efficient pedestrian detection in smart traffic environments.

## Rationale for Selecting YOLOv8 and RT-DETR

YOLOv8 and RT-DETR were selected for their high accuracy, real-time performance, and adaptability to diverse conditions. Prior research has shown both models to perform well under varied lighting and weather scenarios ([Alimov & Meiramkhanov, 2024](#ref-alimov)), making them well-suited for urban pedestrian detection tasks.

Through this selection, our research builds upon proven, cutting-edge detection architectures to analyze robustness, transferability, and explainability in pedestrian detection tasks.

# Dataset

We selected the [EuroCity Persons (ECP)](https://eurocity-dataset.tudelft.nl/) dataset for our experiments. ECP features street-level city images from a driver's point of view in European cities. Given the large scale of the dataset, we decided to focus only on the validation part for our experiments: 10 GB and 4266 image-label pairs. We randomly split the images and corresponding annotations from all the cities into the train, validation, and test sets in a 70-10-20 ratio.

# Model Training

[Ulitralytics PyPI package](https://pypi.org/project/ultralytics/) offers both `YOLOv8` and `RT-DETR` models pre-trained on COCO 2017 dataset for object detection [lin2015microsoft](will insert bibtex citation here later). Although the dataset contains a label for people, it also contains 79 other diverse labels. We narrow down the capabilities of the models by fine-tuning them on a subset from the ECP validation dataset. To compare the models in fair conditions, we decided to use the models of large size (`YOLOv8l`, `RT-DETR-L`) and give each model 2.5 hours of fine-tuning time with a Tesla P100 GPU. So, both models were fine-tuned under time-consistent training conditions: **YOLOv8** for **50 epochs** and **RT-DETR** for **35 epochs**.

To speed up the training, the images were resized to fit a 640x640 pixels square with a preserved aspect ratio. The rest of the image was dynamically padded with grey pixels to fill in the square. The rest of the parameters were set to [default](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters). A default data augmentation was applied with the most notable being HSV colour adjustments, mosaic composition, image erasing, and croping.

# Performence Evaluation

We compare the predictive performance of `YOLOv8l` and `RT-DETR-L`, using standard evaluation metrics(precision, recall, F1-score, and mean average precision (mAP) at different intersection over union (IoU) thresholds). We use this combination of metrics to capture different aspects of model performance: precision and recall for detection quality, F1-score for balance between them, and mAP across IoU thresholds for overall localization accuracy. The results are presented in [Table 1](#table-1).

**Table 1** <a id="table-1"></a>: A comparison of `YOLOv8l` and `RT-DETR-L` models performance.

| Metric               | YOLOv8     | RT-DETR    |
| -------------------- | ---------- | ---------- |
| mAP at 0.05-0.95 IoU | 0.4546     | **0.4551** |
| mAP at 0.50 IoU      | 0.7273     | **0.7628** |
| mAP at 0.75 IoU      | **0.4776** | 0.4679     |
| Precision            | **0.8129** | 0.8033     |
| Recall               | 0.6514     | **0.6706** |
| F1-score             | 0.7233     | **0.7310** |

[//]: # (The mean average precision &#40;mAP&#41; at 0.05-0.95 IoU thresholds is nearly identical for both models, with RT-DETR being slightly better. In the case of mAP at 0.50 IoU, RT-DETR shows a slightly superior predictive performance &#40;0.7628 vs. 0.7273&#41;. However, for mAP at 0.75 IoU, YOLOv8 achieves a higher value &#40;0.4776 vs. 0.4679&#41;. This suggests that YOLOv8 is more effective at pinpointing people with higher localization accuracy while RT-DETR is more effective at globally identifying pedestrians with more relaxed thresholds.)

[//]: # ()
[//]: # (In terms of precision, YOLOv8 outperforms RT-DETR &#40;0.8129 vs. 0.8033&#41;, meaning it generates fewer boxes with no people on average. However, RT-DETR achieves a higher recall &#40;0.6706 vs. 0.6514&#41; that shows the model's ability to identify pedestrians better in this setting. The precision-recall trade-off is reflected in the F1-score, where the results are on par with RT-DETR leading by a small margin.)

[//]: # ()
[//]: # (Overall, the results show RT-DETR leads in terms of recall and general detection performance &#40;mAP at 0.05-0.95&#41; which makes it a better candidate for detecting people in crowded areas. In contrast, YOLOv8 surpasses RT-DETR in more precise pedestrian detection, according to precision and mAP at 0.75, making it a good fit for a more accurate localization of detected people.)

The mean average precision (mAP) across IoU thresholds (0.05–0.95) is similar for both models, with RT-DETR slightly ahead. At 0.50 IoU, RT-DETR performs better (0.7628 vs. 0.7273), while YOLOv8 leads at 0.75 IoU (0.4776 vs. 0.4679), suggesting better localization accuracy.

YOLOv8 also has higher precision (0.8129 vs. 0.8033), producing fewer false positives, whereas RT-DETR achieves better recall (0.6706 vs. 0.6514), indicating stronger overall detection. Their F1-scores are close, with RT-DETR holding a slight edge.

So, RT-DETR excels at broad pedestrian detection, especially in crowded scenes, while YOLOv8 is more precise, making it preferable when accurate localization is critical. Given that the models have comparable performance, we propose the experiments from the next section to better understand and compare their capabilities in three different domains: robustness, transfer learning, and explainability.


# Experiments

In this section we propose a series of experiments, which will test the `YOLOv8` and `RT-DETR` models in three different domains: robustness, transfer learning, and explainability.

## Augmentation Methods for testing robustness

Real-world conditions rarely match the training data. Augmenting images to simulate fog, rain, overexposure, and night-time conditions helps:

- Identify scenarios where models struggle.
- Improve model performance in varied environments.
- Critical for applications like autonomous driving and surveillance thus increasing safety.

We have applied all the mentioned augmentations and applied them to 100 images from the testing set - thus generating 5 testset subsets with different augmentations to evaluate and compare the robustness of the finetuned models. The example augmentation can be seen in [Figure 1](#figure-1).

!["Augmentations vizualization"]({{ '/public/images/augmented_images.png' | relative_url }})

**Figure 1** <a id="figure-1"></a>: Example augmentations applied to images to simulate fog, rain, overexposure, and night-time conditions.

The implementation of the augmentations was inspired by Kou, E., & Curran, N. (2024). _Enhancing autonomous vehicle perception in adverse weather through image augmentation during semantic segmentation training_. Retrieved from [https://github.com/BubblyBingBong/AugmentationSegmentation](https://github.com/BubblyBingBong/AugmentationSegmentation). The details of the implementation are described bellow:

### 1. Fog Simulation

Fog reduces scene contrast and detail. Testing with foggy images can evaluate if the models can detect pedestrians even under low-visibility conditions.

The fog augmentation was implemented in a following way:

- **White Overlay:**
  Create a white image where every pixel is set to 255:

  $$ \text{fog_layer}(x,y) = 255 $$

- **Linear Blending:**
  Blend the original image with the white layer:

$$ \text{fogged_pixel} = (1 - \text{fog_intensity}) \times \text{original_pixel} + \text{fog_intensity} \times 255 $$

- **Gaussian Blur:**
  $$ I*{\text{blur}}(x,y) = \sum*{u,v} I(x-u, y-v) \, G(u,v) $$

  where $ G(u,v) $ is the Gaussian kernel.

### 2. Rain Simulation

Rain introduces dynamic noise and streaks that can occlude parts of pedestrians. Testing with rainy images improves robustness against weather-induced artifacts.

The rain augmentation was implemented in the following way:

- **Rain Streaks Generation:**

  - Create a black layer.
  - For a number of streaks proportional to `rain_intensity`:

    - Randomly select a starting point $(x, y)$.
    - Choose a random length (15–25 pixels).
    - Find end coordinates

      $$ \text{end_x} = x + \text{length} \times \sin(\theta), \quad $$
      $$ \text{end_y} = y + \text{length} \times \cos(\theta) $$

      with $\theta$ between 0.2 and 0.5 radians.

    - Draw a white line representing a rain streak (thickness 2 pixels).

- **Motion Blur:**
  Convolve the rain layer with a 7×7 motion blur kernel:

  $$\text{kernel}[i,j] = \begin{cases} \frac{1}{7} & \text{if } i = 3 \\ 0 & \text{otherwise} \end{cases}$$

  This spreads the intensity horizontally, simulating falling rain. By experimenting we discovered that this looks more realistic.

- **Blending:**
  Blend the rain layer with the original image:

  $$\text{final\_pixel} = 0.7 \times \text{original\_pixel} + 0.3 \times \text{rain\_pixel}$$

### 3. Overexposure Simulation

Overexposure saturates image details. Testing with overexposed images ensures models can detect features even when details are lost.

The overexposure augmentation was implemented in the following way:

- **White Blending:**
  Blend the image with a white image:

  $$ \text{overexposed_pixel} = \text{original_pixel} + \text{exposure_intensity} \times 255 $$

  This brightens the image and washes out details.

- **Gaussian Blur:**
  Apply a smaller Gaussian blur (5×5) to simulate loss of detail because.

### 4. Night Simulation

Night scenes have very low brightness and the colors are a bit different. Testing with night-simulated images can evaluate if the low lighting conditions are also acceptable for the model to perform well.

The night augmentation was implemented in the following way:

- **Brightness Reduction:**
  Convert the image to HSV and reduce the brightness (value channel) by:

  $$ v\_{\text{new}} = \text{clip}(v \times (1 - \text{night_intensity}), 0, 255) $$

- **Blue Tint:**
  Blend the darkened image with a blue mask:

  $$ \text{night_pixel} = 0.9 \times \text{darkened_pixel} + 0.1 \times (0, 0, 30) $$

- **Gaussian Noise:**
  Add noise sampled from a normal distribution $ \mathcal{N}(0,10) $ and clip:

  $$ \text{night_pixel} = \text{clip}(\text{night_pixel} + \text{noise}, 0, 255) $$

## Transfer Learning on Traffic Sign Detection Dataset

To assess generalization, we applied transfer learning from pedestrian to traffic sign detection, a related task within the same visual domain. Both are key to autonomous driving and share challenges like cluttered scenes, small objects, and real-time constraints. The shared road context may further support model transferability.

We used the publicly available [Traffic Sign Detection](https://universe.roboflow.com/selfdriving-car-qtywx/self-driving-cars-lfjou) dataset for this experiment.


### Traffic Sign Detection Dataset

This dataset contains **4,969 traffic sign images**, divided into:

- **Train Set**: 3,530 images (71%)
- **Validation Set**: 801 images (16%)
- **Test Set**: 638 images (13%)

**Preprocessing**: Images are resized to **416x416**
**Augmentations**: **None** applied

**Classes**: 15 total, including:
`Green Light`, `Red Light`, `Speed Limit 10`, `Speed Limit 20`, `Speed Limit 30`, `Speed Limit 40`, `Speed Limit 50`, `Speed Limit 60`, `Speed Limit 70`, `Speed Limit 80`, `Speed Limit 90`, `Speed Limit 100`, `Speed Limit 110`, `Speed Limit 120`, `Stop`

### Full Fine-Tuning vs Partial Fine-Tuning on Traffic Sign Detection Dataset

To explore the efficiency and effectiveness of transfer learning, we compared three fine-tuning strategies on the traffic sign detection task:

- **Full Fine-Tuning**: All layers of the pre-trained model (on pedestrian dataset) are updated during training.
- **Partial Fine-Tuning**: Only the final layers of the pre-trained model (on pedestrian dataset) are updated, while the backbone layers remain frozen.
- **Default (Ultralytics Weights)**: Models are fine-tuned directly using the default COCO-pretrained weights provided by the Ultralytics framework, without any manual layer freezing.

Including the default-weight models is important, as it allows us to assess whether the general-purpose features learned from COCO are already sufficient for strong performance in a related domain, and whether the training in the same setting can be an advantage. Moreover, comparing models with frozen and unfrozen backbones helps us evaluate the trade-off between training cost and performance.

As in the pedestrian detection task, we fine-tuned both models under consistent training conditions:
- **YOLOv8** was trained for **50 epochs**
- **RT-DETR** was trained for **35 epochs**



# Results and Discussion

## Augmentation Methods for testing robustness

The results shows that the robustness of RT-DETR and YOLO is environmentally conditioned. (See [Figure 2](#figure-2)) Under normal, well-lit conditions, RT-DETR outperforms YOLO on nearly all metrics, including mAP, precision, and recall, suggesting better detection accuracy under good visibility conditions. Under foggy weather, although both models have comparable mAP@0.50 scores, YOLO has a slight advantage over RT-DETR on stricter metrics (mAP@0.50–0.95, mAP@0.75, precision, and recall), which can be possibly explained by the fact that YOLO's feature extraction is more capable of dealing with low-contrast images. Under rainy conditions, RT-DETR performs better with much higher mAP values and better precision and recall, showing a good ability to deal with image distortions and occlusions (like rain strokes). Overexposure show little variation between models. In night or low-light settings, YOLO is more robust and outperforms RT-DETR in every metric. While RT-DETR is more robust in rain and well-lit scenes and performs better, YOLO is the more stable under hard weather conditions such as fog and especially in low-light or night-time conditions.

What is important to note is that finetuning the models using the described augmentation methods would potentially enhance the performance in various conditions thus making the models more robust. We recommend this as a direction for future research.

![Robustness Comparison]({{ '/public/images/metrics_robustness.png' | relative_url }})
**Figure 2** <a id="figure-2"></a>: Robustness comparison between RT-DETR and YOLO under various environmental conditions, including fog, rain, overexposure, and night-time scenarios.

## Transfer Learning on Traffic Sign Detection Dataset

We explored how well RT-DETR and YOLO adapt to a new task—*Traffic Sign Detection—using **full* and *partial fine-tuning* strategies. Below, we compare performance across different validation metrics. Additionally, we fine-tune the default models from ultralytics. This allows us to examine whether models pre-trained on general object categories (using COCO weights) can adapt quicker then the previously trained models.

### Training Metrics Over Epochs

The [Figure 3](#figure-3) illustrates how the models perform over the course of training epochs. RT-DETR initialized with the default Ultralytics weights achieves best performance early on and consistently outperforms all other configurations across every metric. It reaches peak values quickly and maintains this lead throughout training. RT-DETR Full Fine-Tune also performs very well, eventually converging to performance levels close to the default version, though it starts off more gradually. Interestingly, the RT-DETR Partial Fine-Tune variant achieves solid early gains but plateaus sooner, ultimately falling behind both the full and default versions. For YOLO, the Full Fine-Tune model demonstrates the most substantial improvement over epochs, steadily increasing across all metrics and surpassing both the Default and Partial Fine-Tune versions. YOLO Full and YOLO Partial Fine-Tune exhibit similar growth patterns early on, with YOLO Full Fine-Tune slightly outperforming YOLO Partial Fine-Tune across all metrics.

![Robustness Comparison]({{ '/public/images/tl_training_epochs.png' | relative_url }})
**Figure 3** <a id="figure-3"></a>: Training metrics (precision, recall, mAP@50, and mAP@50-95) over epochs for all fine-tuning strategies.

### Training Metrics Over Time

[Figure 4](#figure-4) displays the same metrics over training time in seconds, offering insights into the efficiency of each approach. RT-DETR Default continues to dominate, achieving high scores rapidly and outperforming all other configurations with minimal training time. RT-DETR Full Fine-Tune starts more slowly but climbs steadily and eventually reaches near-optimal performance. However, RT-DETR Partial Fine-Tune, despite an early surge, levels off and falls behind both full and default configurations as training continues. In the YOLO models, Default Fine-Tune shows the best long-term gains, gradually overtaking the other variants as time progresses. Meanwhile, YOLO Full and YOLO Partial Fine-Tune perform comparably, with YOLO Full slightly outperforming Partial Fine-Tune in the end for all metrics, possibly because of longer training time (for the same number of epochs).

![Robustness Comparison]({{ '/public/images/tl_training_time.png' | relative_url }})
**Figure 4** <a id="figure-4"></a>: Training metrics plotted over time (in seconds) to evaluate training efficiency.

# Conclusion

In this part, we summarize our findings by briefly answering the research questions.

## Augmentation Methods for testing robustness

Our experiments show that RT-DETR performs better in rain and well-lit conditions, while YOLOv8 is more robust in fog and night-time scenarios. This suggests that the choice of model should be guided by expected environmental conditions. Both models could benefit from further fine-tuning with augmented data to improve performance under adverse conditions.

## Transfer Learning on Traffic Sign Detection Dataset

The results indicate that while it is possible to transfer knowledge gained from pedestrian detection to traffic sign detection, this approach is neither as effective nor as efficient as using default (COCO-pretrained) weights. Models fine-tuned from COCO weights adapt more quickly and often achieve better performance than those starting from pedestrian-trained checkpoints, suggesting that a broader, more diverse pretraining domain can offer a stronger foundation for new detection tasks.

## Explainability

# References

- <a id="ref-carion"></a>Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020). *End-to-End Object Detection with Transformers*. arXiv preprint arXiv:2005.12872. [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872)
- <a id="ref-luo"></a>Luo, X., Shao, B., Cai, Z., & Wang, Y. (2024). *A lightweight YOLOv5-FFM model for occlusion pedestrian detection*. arXiv preprint arXiv:2408.06633. [https://arxiv.org/abs/2408.06633](https://arxiv.org/abs/2408.06633)
- <a id="ref-peng"></a>Peng, H., & Chen, S. (2024). FedsNet: The real-time network for pedestrian detection based on RT-DETR. *Journal of Real-Time Image Processing, 21*, 142. [https://doi.org/10.1007/s11554-024-01523-8](https://doi.org/10.1007/s11554-024-01523-8)
- <a id="ref-redmon"></a>Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). *You Only Look Once: Unified, Real-Time Object Detection*. arXiv preprint arXiv:1506.02640. [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)
- <a id="ref-sadik"></a>Sadik, M. N., Hossain, T., & Sayeed, F. (2024). *Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep Learning*. arXiv preprint arXiv:2404.08081. [https://arxiv.org/abs/2404.08081](https://arxiv.org/abs/2404.08081)
- <a id="ref-alimov"></a>Alimov, M., & Meiramkhanov, T. (2024). Domain Generalization in Autonomous Driving: Evaluating YOLOv8s, RT-DETR, and YOLO-NAS with the ROAD-Almaty Dataset. arXiv preprint arXiv:2412.12349. [https://arxiv.org/abs/2412.12349](https://arxiv.org/abs/2412.12349)
- <a id="ref-jocher"></a>Jocher, G., et al. (2023). *YOLO by Ultralytics*. [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)
- <a id="ref-zhang"></a>Zhang, X., Liu, X., Zhang, Y., Wang, Y., Lu, C., & Qiao, Y. (2023). *RT-DETR: Real-Time Detection Transformer*. *arXiv preprint* arXiv:2304.08069. [https://arxiv.org/abs/2304.08069](https://arxiv.org/abs/2304.08069)
